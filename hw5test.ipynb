sentences = ["What is the best time to call you tomorrow?",  "What is the best hour to call you tomorrow?", "What is the best phone number to reach you at tomorrow?", "How many phones will be in your possession tomorrow?", "Where would you like to meet tomorrow?", "How long do you typically like to stay on the phone?", "Do you have a house or a mobile phone number?"]

import numpy as np
from collections import Counter

## just a little bit of preprocessing for our data, counting/ordering types
data = {'sentences': [s for x in sentences
                      for s in [(x).lower().split()]]}  #the sentences

for i in range(len(data['sentences'])):
    data['sentences'][i] = [e for i in data['sentences'][i] for e in [i, " "]][:-1]


data['counts'] = Counter([t for s in data['sentences'] for t in s]) #unique words and their counts
data['word2index'] = {t: i for i, t in enumerate(data['counts'])}  #indices for each word
data['index2word'] = {v: k for k, v in data['word2index'].items()}

import torch
WINDOW_SIZE = 5  #TODO:  Play with this
half = WINDOW_SIZE//2
Xtrain = []
Ytrain = []

for s in data['sentences']:  #for each sentence
    for i in range(0,len(s)-WINDOW_SIZE+1): #for each word in sentence
        T = [data['word2index'][s[j]] for j in range(i,i+WINDOW_SIZE)]  #grab the word indices for the window
        Ytrain.append([T[half]])
        Xtrain.append(T[:half]+T[half+1:])

Xtrain = torch.tensor(Xtrain, dtype=torch.long)
Ytrain = torch.tensor(Ytrain, dtype=torch.long)

from torch import nn
VOCAB_SIZE = len(data['counts'])
EMBED_DIMENSION = 300  #TODO:  Play with this

class CBOW_Model(nn.Module):
    def __init__(self, vocab_size, embed_dims):
        super().__init__()
        self.embedded_dim = embed_dims
        
        self.embeddings = nn.Linear(vocab_size, embed_dims)
        self.linear = nn.Linear(
            in_features=embed_dims,
            out_features=vocab_size,
        )
        
    def forward(self, inputs_):
        x = torch.zeros(inputs_.shape[0],inputs_.shape[1],self.embedded_dim)  #shape of output.  (samples x items in sample x embedded dims)
        
        for i in range(inputs_.shape[0]):  #for each training sample
            x[i] = self.embeddings(inputs_[i])  #get its embeddings for the words in the sample
        
        x = x.mean(axis=1)  #take the average embedding
        x = self.linear(x)   #predict
        return x

model = CBOW_Model(VOCAB_SIZE, EMBED_DIMENSION)

MAX_EPOCHS = 1000 #TODO:  Play with this

#Choose your loss function and optimizer

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adagrad(model.fc.parameters(), lr=0.01)

model.train()
running_loss = []

XOneHot = torch.nn.functional.one_hot(Xtrain).to(torch.float)
for epoch in range(MAX_EPOCHS):

    optimizer.zero_grad()
    outputs = model(XOneHot)
   
    loss = loss_fn(outputs, Ytrain.squeeze())
    loss.backward()
    optimizer.step()

    running_loss.append(loss.item())


#TODO:  Visualize the training process

weights = list(model.parameters())[0].detach().numpy().T   #Get the weights from the embedded layer
ids = torch.nn.functional.one_hot(torch.arange(VOCAB_SIZE))  #The indices of all the words
embeddings = ids@weights  # The embeddings of the words

# normalization
norms = (embeddings ** 2).sum(axis=1) ** (1 / 2) + 10**(-10)
norms = np.reshape(norms, (len(norms), 1))
embeddings_norm = embeddings / norms


#This function takes a word (as a string), gets its embedding, compares to all other embeddings via cosine similarity.
#Then returns the topN
def get_top_similar(word: str, topN: int = 10):
    try:
        word_id = data['word2index'][word]   #This word's ID
    except:
        print("Out of vocabulary word")
        return

    word_vec = embeddings_norm[word_id]   #This word's normalized embedding
        
    word_vec = np.reshape(word_vec, (len(word_vec), 1))
    dists = np.matmul(embeddings_norm, word_vec).flatten()  #Dot product with all the other embeddings
    topN_ids = np.argsort(-dists)[1 : topN + 1]  #Sort by most similar

    topN_dict = {}
    for sim_word_id in topN_ids:
        sim_word = data['index2word'][sim_word_id.item()]
        topN_dict[sim_word] = dists[sim_word_id]
    return topN_dict

myword = "time"
try:
    for word, sim in get_top_similar(myword).items():
        print("{}: {:.3f}".format(word, sim))
except:
    print("Word doesn't exist")